{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "azml-taejun.minwoo/RNN-1.ipynb",
      "provenance": [],
      "mount_file_id": "131qBk9i__aC2qzYZZ30O1Ub6V8TRg4Bq",
      "authorship_tag": "ABX9TyN3bF/mSeU6OgfqX9RnIZkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marlrero/AI_PrimaryEducation/blob/main/azml_taejun_minwoo_RNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so5aQQKx9cNg"
      },
      "source": [
        "\r\n",
        "### Keras RNN으로 여행자 수 예측 - 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReS5tgNsWo0Z"
      },
      "source": [
        "원래 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFJtFKyaWoWX",
        "outputId": "817e126f-32e0-4b78-fab1-891ab59a59b1"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from time import time\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler # 최대-최소 정규화\r\n",
        "from keras.layers import Dense, LSTM, InputLayer # LSTM(Long-Short Term Memory)\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "# hyperparameters\r\n",
        "MY_PAST = 12\r\n",
        "MY_SHAPE = (MY_PAST, 1)\r\n",
        "MY_HIDDEN = 300\r\n",
        "\r\n",
        "MY_SPLIT = 0.8\r\n",
        "MY_EPOCH = 300\r\n",
        "MY_BATCH = 64\r\n",
        "\r\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset/airline.csv', header=None, usecols=[1])\r\n",
        "\r\n",
        "# Normalization\r\n",
        "scaler = MinMaxScaler()\r\n",
        "raw_DB = scaler.fit_transform(df) # pandas dataframe -> numpy array\r\n",
        "print(raw_DB.flatten()) # 1차원 배열로 펼침\r\n",
        "\r\n",
        "# 데이터 분할\r\n",
        "data = []\r\n",
        "\r\n",
        "# 0 ~ 전체 데이터 길이 - MY_PAST - 1까지\r\n",
        "for i in range(len(raw_DB) - MY_PAST):\r\n",
        "  data.append(raw_DB[i:i + MY_PAST + 1])\r\n",
        "\r\n",
        "# data를 numpy로 전환한 후\r\n",
        "print(type(data)) \r\n",
        "data = np.array(data)\r\n",
        "print(type(data))\r\n",
        "\r\n",
        "# 묶음 데이터를 섞고\r\n",
        "np.random.shuffle(data) \r\n",
        "  # 데이터를 묶었기 때문에, 시계열 데이터 순서가 망가지지 않음\r\n",
        "  # 위에서 묶음 안에서 시계열은 그대로 유지되기 때문\r\n",
        "\r\n",
        "# 입력과 출력 분할\r\n",
        "print(\"분할 전 데이터 형상:\", data.shape) # (132*13) -> (132*13*1->한 값이 하나의 차원이 됨)\r\n",
        "X_data = data[:, 0:MY_PAST] # 모든 행에 대해 0 ~ MY_PAST - 1까지 뽑기\r\n",
        "Y_data = data[:, -1] # 모든 행에 대해 마지막 열 뽑기\r\n",
        "\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=1.0 - MY_SPLIT)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(InputLayer(input_shape=MY_SHAPE)) # 12*1\r\n",
        "model.add(LSTM(units=MY_HIDDEN))\r\n",
        "model.add(Dense(units=1, activation=\"sigmoid\"))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop', loss='mape', metrics=['acc'])\r\n",
        "\r\n",
        "begin = time()\r\n",
        "# 132개 밖에 안되서 batch_size 사용 안함\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=MY_EPOCH, verbose=0)\r\n",
        "end = time()\r\n",
        "print(\"Learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print('Total Loss:', score[0])\r\n",
        "#print('Accuracy:', score[1]) # Regression에서는 정확도 의미없음."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01544402 0.02702703 0.05405405 0.04826255 0.03281853 0.05984556\n",
            " 0.08494208 0.08494208 0.06177606 0.02895753 0.         0.02702703\n",
            " 0.02123552 0.04247104 0.07142857 0.05984556 0.04054054 0.08687259\n",
            " 0.12741313 0.12741313 0.1042471  0.05598456 0.01930502 0.06949807\n",
            " 0.07915058 0.08880309 0.14285714 0.11389961 0.13127413 0.14285714\n",
            " 0.18339768 0.18339768 0.15444015 0.11196911 0.08108108 0.11969112\n",
            " 0.12934363 0.14671815 0.17181467 0.14864865 0.15250965 0.22007722\n",
            " 0.24324324 0.26640927 0.2027027  0.16795367 0.13127413 0.17374517\n",
            " 0.17760618 0.17760618 0.25482625 0.25289575 0.24131274 0.26833977\n",
            " 0.30888031 0.32432432 0.25675676 0.20656371 0.14671815 0.18725869\n",
            " 0.19305019 0.16216216 0.25289575 0.23745174 0.25096525 0.30888031\n",
            " 0.38223938 0.36486486 0.2992278  0.24131274 0.19111969 0.24131274\n",
            " 0.26640927 0.24903475 0.31467181 0.31853282 0.32046332 0.40733591\n",
            " 0.5019305  0.46911197 0.4015444  0.32818533 0.25675676 0.33590734\n",
            " 0.34749035 0.33397683 0.41119691 0.4034749  0.41312741 0.52123552\n",
            " 0.5965251  0.58108108 0.48455598 0.38996139 0.32239382 0.38996139\n",
            " 0.40733591 0.38030888 0.48648649 0.47104247 0.48455598 0.61389961\n",
            " 0.6969112  0.7007722  0.57915058 0.46911197 0.38803089 0.44787645\n",
            " 0.45559846 0.41312741 0.4980695  0.47104247 0.5        0.63899614\n",
            " 0.74710425 0.77413127 0.57915058 0.49227799 0.3976834  0.44980695\n",
            " 0.49420849 0.45945946 0.58301158 0.56370656 0.61003861 0.71042471\n",
            " 0.85714286 0.87837838 0.69305019 0.58494208 0.4980695  0.58108108\n",
            " 0.6042471  0.55405405 0.60810811 0.68918919 0.71042471 0.83204633\n",
            " 1.         0.96911197 0.77992278 0.68918919 0.55212355 0.63320463]\n",
            "<class 'list'>\n",
            "<class 'numpy.ndarray'>\n",
            "분할 전 데이터 형상: (132, 13, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 300)               362400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 301       \n",
            "=================================================================\n",
            "Total params: 362,701\n",
            "Trainable params: 362,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Learning time: 11.93sec\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 15.6437 - acc: 0.0000e+00\n",
            "Total Loss: 15.64366340637207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka6jnrYL9cfd"
      },
      "source": [
        "1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDcyCsFo9YVR",
        "outputId": "6a6da6c2-2d77-4c93-9888-8238d69e58dc"
      },
      "source": [
        "# 추가\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "####################################\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from time import time\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler # 최대-최소 정규화\r\n",
        "from keras.layers import Dense, LSTM, InputLayer # LSTM(Long-Short Term Memory)\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "# hyperparameters\r\n",
        "MY_PAST = 12\r\n",
        "MY_SHAPE = (MY_PAST, 1)\r\n",
        "MY_HIDDEN = 300\r\n",
        "\r\n",
        "MY_SPLIT = 0.8\r\n",
        "MY_EPOCH = 300\r\n",
        "MY_BATCH = 64\r\n",
        "\r\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset/airline.csv', header=None, usecols=[1])\r\n",
        "\r\n",
        "# Normalization\r\n",
        "scaler = MinMaxScaler()\r\n",
        "raw_DB = scaler.fit_transform(df) # pandas dataframe -> numpy array\r\n",
        "\r\n",
        "# 데이터 분할\r\n",
        "data = []\r\n",
        "\r\n",
        "# 0 ~ 전체 데이터 길이 - MY_PAST - 1까지\r\n",
        "for i in range(len(raw_DB) - MY_PAST):\r\n",
        "  data.append(raw_DB[i:i + MY_PAST + 1])\r\n",
        "\r\n",
        "  # data를 numpy로 전환한 후\r\n",
        "print(type(data)) \r\n",
        "data = np.array(data)\r\n",
        "print(type(data))\r\n",
        "\r\n",
        "# 묶음 데이터를 섞고\r\n",
        "np.random.shuffle(data) \r\n",
        "  # 데이터를 묶었기 때문에, 시계열 데이터 순서가 망가지지 않음\r\n",
        "  # 위에서 묶음 안에서 시계열은 그대로 유지되기 때문\r\n",
        "\r\n",
        "# 입력과 출력 분할\r\n",
        "print(\"분할 전 데이터 형상:\", data.shape) # (132*13) -> (132*13*1->한 값이 하나의 차원이 됨)\r\n",
        "X_data = data[:, 0:MY_PAST] # 모든 행에 대해 0 ~ MY_PAST - 1까지 뽑기\r\n",
        "Y_data = data[:, -1] # 모든 행에 대해 마지막 열 뽑기\r\n",
        "\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=1.0 - MY_SPLIT)\r\n",
        "\r\n",
        "def create_model(units=MY_HIDDEN, optimizer='rmsprop', loss='mape'):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(InputLayer(input_shape=MY_SHAPE)) # 12*1\r\n",
        "    model.add(LSTM(units=units))\r\n",
        "    model.add(Dense(units=1, activation=\"sigmoid\"))\r\n",
        "    model.compile(optimizer=optimizer, loss=loss)\r\n",
        "    return model\r\n",
        "\r\n",
        "param_grid = [ \r\n",
        "        {   'units': [100, 200, 300, 400],\r\n",
        "            'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adam'], 'loss': ['mse', 'mae', 'mape'],\r\n",
        "         },\r\n",
        "      ]\r\n",
        "\r\n",
        "model = KerasRegressor(build_fn=create_model, epochs=MY_EPOCH, verbose=0)\r\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\r\n",
        "\r\n",
        "begin = time()\r\n",
        "print(X_train.shape, Y_train.shape)\r\n",
        "grid_result = grid.fit(X=X_train, y=Y_train)\r\n",
        "\r\n",
        "#model.fit(x=X_train, y=Y_train, epochs=MY_EPOCH, verbose=2)\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "print(grid.best_score_, grid.best_params_)\r\n",
        "print(\"Test Score:\", grid_result.score(X_test, Y_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'numpy.ndarray'>\n",
            "분할 전 데이터 형상: (132, 13, 1)\n",
            "(105, 12, 1) (105, 1)\n",
           
            "learning time: 1118.32sec\n",
            "-0.0016401301603764296 {'loss': 'mse', 'optimizer': 'Adam', 'units': 400}\n",
            
            "Test Score: -0.0016580322990193963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wz94pMm_9Af"
      },
      "source": [
        "2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raKjLV2Q_9Ty",
        "outputId": "929873e5-536f-4260-e217-bd776257001b"
      },
      "source": [
        "# 추가\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "####################################\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from time import time\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler # 최대-최소 정규화\r\n",
        "from keras.layers import Dense, LSTM, InputLayer # LSTM(Long-Short Term Memory)\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "# hyperparameters\r\n",
        "MY_PAST = 12\r\n",
        "MY_SHAPE = (MY_PAST, 1)\r\n",
        "MY_HIDDEN = 300\r\n",
        "\r\n",
        "MY_SPLIT = 0.8\r\n",
        "MY_EPOCH = 300\r\n",
        "MY_BATCH = 64\r\n",
        "\r\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset/airline.csv', header=None, usecols=[1])\r\n",
        "\r\n",
        "# Normalization\r\n",
        "scaler = MinMaxScaler()\r\n",
        "raw_DB = scaler.fit_transform(df) # pandas dataframe -> numpy array\r\n",
        "\r\n",
        "# 데이터 분할\r\n",
        "data = []\r\n",
        "\r\n",
        "# 0 ~ 전체 데이터 길이 - MY_PAST - 1까지\r\n",
        "for i in range(len(raw_DB) - MY_PAST):\r\n",
        "  data.append(raw_DB[i:i + MY_PAST + 1])\r\n",
        "\r\n",
        "  # data를 numpy로 전환한 후\r\n",
        "print(type(data)) \r\n",
        "data = np.array(data)\r\n",
        "print(type(data))\r\n",
        "\r\n",
        "# 묶음 데이터를 섞고\r\n",
        "np.random.shuffle(data) \r\n",
        "  # 데이터를 묶었기 때문에, 시계열 데이터 순서가 망가지지 않음\r\n",
        "  # 위에서 묶음 안에서 시계열은 그대로 유지되기 때문\r\n",
        "\r\n",
        "# 입력과 출력 분할\r\n",
        "print(\"분할 전 데이터 형상:\", data.shape) # (132*13) -> (132*13*1->한 값이 하나의 차원이 됨)\r\n",
        "X_data = data[:, 0:MY_PAST] # 모든 행에 대해 0 ~ MY_PAST - 1까지 뽑기\r\n",
        "Y_data = data[:, -1] # 모든 행에 대해 마지막 열 뽑기\r\n",
        "\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=1.0 - MY_SPLIT)\r\n",
        "\r\n",
        "def create_model():\r\n",
        "    model = Sequential()\r\n",
        "    model.add(InputLayer(input_shape=MY_SHAPE)) # 12*1\r\n",
        "    model.add(LSTM(units=400)) # unit 400\r\n",
        "    model.add(Dense(units=1, activation=\"sigmoid\"))\r\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['acc']) # adam, mse\r\n",
        "    return model\r\n",
        "  \r\n",
        "param_grid = [ \r\n",
        "        {'epochs': [100, 200, 300, 500], },\r\n",
        "      ]\r\n",
        "\r\n",
        "model = KerasRegressor(build_fn=create_model, verbose=0)\r\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\r\n",
        "\r\n",
        "# 교차엔트로피 손실 값과 정확도를 넘겨줌\r\n",
        "begin = time()\r\n",
        "grid_result = grid.fit(X=X_train, y=Y_train)\r\n",
        "#model.fit(x=X_train, y=Y_train, epochs=MY_EPOCH, batch_size=MY_BATCH, verbose=0)\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "print(grid.best_score_, grid.best_params_)\r\n",
        "\r\n",
        "print(grid_result.score(X_test, Y_test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'numpy.ndarray'>\n",
            "분할 전 데이터 형상: (132, 13, 1)\n",
            
            "learning time: 109.69sec\n",
            "-0.0015431056963279843 {'epochs': 500}\n",
            
            "-0.0013353683752939105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya_4RND4voI6"
      },
      "source": [
        "당연히 epoch를 많이 한게 좋은 결과를 얻을것임."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz43NTYivmtd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
