{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMUUz8zz74Eua6xNkynxVzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marlrero/AI_PrimaryEducation/blob/main/fashion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8L7PJsVXUNW"
      },
      "source": [
        "### Keras CNN으로 패션 아이템 구분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3fXk0jFXT7m"
      },
      "source": [
        "#### 1. import package & set hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPxQV1bgXSM_"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from time import time\r\n",
        "import numpy as np\r\n",
        "from keras.datasets import fashion_mnist # dataset\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.utils import np_utils # One-hot encoding\r\n",
        "from keras.layers import Conv2D, Dense, Flatten, InputLayer, MaxPool2D\r\n",
        "\r\n",
        "# hyperparameter\r\n",
        "MY_EPOCH = 10\r\n",
        "MY_BATCH = 500"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-dReOGCcSeI"
      },
      "source": [
        "#### 2. load dataset & Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HogOdRA0YAhM",
        "outputId": "f9716760-6537-488c-c4b2-4699a489629f"
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "\r\n",
        "# 훈련 데이터, 훈련 레이블, 검증 데이터, 검증 레이블\r\n",
        "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tTzksoXjX_yM",
        "outputId": "722211f0-a9c9-4219-ed2f-7764400891b9"
      },
      "source": [
        "print(\"첫번째 데이터의 화소정보:\", X_train[0])\r\n",
        "print(\"첫번째 데이터의 라벨정보:\", Y_train[0])\r\n",
        "\r\n",
        "plt.imshow(X_train[0], cmap='gray')\r\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "첫번째 데이터의 화소정보: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "첫번째 데이터의 라벨정보: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l62QmZWHdzZk",
        "outputId": "35c7e8fc-4e4b-462b-bc24-3e5a76c41d1a"
      },
      "source": [
        "# 0 ~ 1 사이의 정규화(Normalization)\r\n",
        "X_train = X_train / 255.0\r\n",
        "X_test - X_test / 255.0\r\n",
        "\r\n",
        "X_train[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
              "        0.05098039, 0.28627451, 0.        , 0.        , 0.00392157,\n",
              "        0.01568627, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.00392157, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01176471, 0.        , 0.14117647,\n",
              "        0.53333333, 0.49803922, 0.24313725, 0.21176471, 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.01176471, 0.01568627,\n",
              "        0.        , 0.        , 0.01176471],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
              "        0.8       , 0.69019608, 0.5254902 , 0.56470588, 0.48235294,\n",
              "        0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.04705882, 0.03921569, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.60784314,\n",
              "        0.9254902 , 0.81176471, 0.69803922, 0.41960784, 0.61176471,\n",
              "        0.63137255, 0.42745098, 0.25098039, 0.09019608, 0.30196078,\n",
              "        0.50980392, 0.28235294, 0.05882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00392157, 0.        , 0.27058824, 0.81176471,\n",
              "        0.8745098 , 0.85490196, 0.84705882, 0.84705882, 0.63921569,\n",
              "        0.49803922, 0.4745098 , 0.47843137, 0.57254902, 0.55294118,\n",
              "        0.34509804, 0.6745098 , 0.25882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.00392157, 0.00392157, 0.        , 0.78431373, 0.90980392,\n",
              "        0.90980392, 0.91372549, 0.89803922, 0.8745098 , 0.8745098 ,\n",
              "        0.84313725, 0.83529412, 0.64313725, 0.49803922, 0.48235294,\n",
              "        0.76862745, 0.89803922, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.71764706, 0.88235294,\n",
              "        0.84705882, 0.8745098 , 0.89411765, 0.92156863, 0.89019608,\n",
              "        0.87843137, 0.87058824, 0.87843137, 0.86666667, 0.8745098 ,\n",
              "        0.96078431, 0.67843137, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.75686275, 0.89411765,\n",
              "        0.85490196, 0.83529412, 0.77647059, 0.70588235, 0.83137255,\n",
              "        0.82352941, 0.82745098, 0.83529412, 0.8745098 , 0.8627451 ,\n",
              "        0.95294118, 0.79215686, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.01176471, 0.        , 0.04705882, 0.85882353, 0.8627451 ,\n",
              "        0.83137255, 0.85490196, 0.75294118, 0.6627451 , 0.89019608,\n",
              "        0.81568627, 0.85490196, 0.87843137, 0.83137255, 0.88627451,\n",
              "        0.77254902, 0.81960784, 0.20392157],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02352941, 0.        , 0.38823529, 0.95686275, 0.87058824,\n",
              "        0.8627451 , 0.85490196, 0.79607843, 0.77647059, 0.86666667,\n",
              "        0.84313725, 0.83529412, 0.87058824, 0.8627451 , 0.96078431,\n",
              "        0.46666667, 0.65490196, 0.21960784],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.01568627,\n",
              "        0.        , 0.        , 0.21568627, 0.9254902 , 0.89411765,\n",
              "        0.90196078, 0.89411765, 0.94117647, 0.90980392, 0.83529412,\n",
              "        0.85490196, 0.8745098 , 0.91764706, 0.85098039, 0.85098039,\n",
              "        0.81960784, 0.36078431, 0.        ],\n",
              "       [0.        , 0.        , 0.00392157, 0.01568627, 0.02352941,\n",
              "        0.02745098, 0.00784314, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.92941176, 0.88627451, 0.85098039,\n",
              "        0.8745098 , 0.87058824, 0.85882353, 0.87058824, 0.86666667,\n",
              "        0.84705882, 0.8745098 , 0.89803922, 0.84313725, 0.85490196,\n",
              "        1.        , 0.30196078, 0.        ],\n",
              "       [0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24313725,\n",
              "        0.56862745, 0.8       , 0.89411765, 0.81176471, 0.83529412,\n",
              "        0.86666667, 0.85490196, 0.81568627, 0.82745098, 0.85490196,\n",
              "        0.87843137, 0.8745098 , 0.85882353, 0.84313725, 0.87843137,\n",
              "        0.95686275, 0.62352941, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
              "        0.17254902, 0.32156863, 0.41960784, 0.74117647, 0.89411765,\n",
              "        0.8627451 , 0.87058824, 0.85098039, 0.88627451, 0.78431373,\n",
              "        0.80392157, 0.82745098, 0.90196078, 0.87843137, 0.91764706,\n",
              "        0.69019608, 0.7372549 , 0.98039216, 0.97254902, 0.91372549,\n",
              "        0.93333333, 0.84313725, 0.        ],\n",
              "       [0.        , 0.22352941, 0.73333333, 0.81568627, 0.87843137,\n",
              "        0.86666667, 0.87843137, 0.81568627, 0.8       , 0.83921569,\n",
              "        0.81568627, 0.81960784, 0.78431373, 0.62352941, 0.96078431,\n",
              "        0.75686275, 0.80784314, 0.8745098 , 1.        , 1.        ,\n",
              "        0.86666667, 0.91764706, 0.86666667, 0.82745098, 0.8627451 ,\n",
              "        0.90980392, 0.96470588, 0.        ],\n",
              "       [0.01176471, 0.79215686, 0.89411765, 0.87843137, 0.86666667,\n",
              "        0.82745098, 0.82745098, 0.83921569, 0.80392157, 0.80392157,\n",
              "        0.80392157, 0.8627451 , 0.94117647, 0.31372549, 0.58823529,\n",
              "        1.        , 0.89803922, 0.86666667, 0.7372549 , 0.60392157,\n",
              "        0.74901961, 0.82352941, 0.8       , 0.81960784, 0.87058824,\n",
              "        0.89411765, 0.88235294, 0.        ],\n",
              "       [0.38431373, 0.91372549, 0.77647059, 0.82352941, 0.87058824,\n",
              "        0.89803922, 0.89803922, 0.91764706, 0.97647059, 0.8627451 ,\n",
              "        0.76078431, 0.84313725, 0.85098039, 0.94509804, 0.25490196,\n",
              "        0.28627451, 0.41568627, 0.45882353, 0.65882353, 0.85882353,\n",
              "        0.86666667, 0.84313725, 0.85098039, 0.8745098 , 0.8745098 ,\n",
              "        0.87843137, 0.89803922, 0.11372549],\n",
              "       [0.29411765, 0.8       , 0.83137255, 0.8       , 0.75686275,\n",
              "        0.80392157, 0.82745098, 0.88235294, 0.84705882, 0.7254902 ,\n",
              "        0.77254902, 0.80784314, 0.77647059, 0.83529412, 0.94117647,\n",
              "        0.76470588, 0.89019608, 0.96078431, 0.9372549 , 0.8745098 ,\n",
              "        0.85490196, 0.83137255, 0.81960784, 0.87058824, 0.8627451 ,\n",
              "        0.86666667, 0.90196078, 0.2627451 ],\n",
              "       [0.18823529, 0.79607843, 0.71764706, 0.76078431, 0.83529412,\n",
              "        0.77254902, 0.7254902 , 0.74509804, 0.76078431, 0.75294118,\n",
              "        0.79215686, 0.83921569, 0.85882353, 0.86666667, 0.8627451 ,\n",
              "        0.9254902 , 0.88235294, 0.84705882, 0.78039216, 0.80784314,\n",
              "        0.72941176, 0.70980392, 0.69411765, 0.6745098 , 0.70980392,\n",
              "        0.80392157, 0.80784314, 0.45098039],\n",
              "       [0.        , 0.47843137, 0.85882353, 0.75686275, 0.70196078,\n",
              "        0.67058824, 0.71764706, 0.76862745, 0.8       , 0.82352941,\n",
              "        0.83529412, 0.81176471, 0.82745098, 0.82352941, 0.78431373,\n",
              "        0.76862745, 0.76078431, 0.74901961, 0.76470588, 0.74901961,\n",
              "        0.77647059, 0.75294118, 0.69019608, 0.61176471, 0.65490196,\n",
              "        0.69411765, 0.82352941, 0.36078431],\n",
              "       [0.        , 0.        , 0.29019608, 0.74117647, 0.83137255,\n",
              "        0.74901961, 0.68627451, 0.6745098 , 0.68627451, 0.70980392,\n",
              "        0.7254902 , 0.7372549 , 0.74117647, 0.7372549 , 0.75686275,\n",
              "        0.77647059, 0.8       , 0.81960784, 0.82352941, 0.82352941,\n",
              "        0.82745098, 0.7372549 , 0.7372549 , 0.76078431, 0.75294118,\n",
              "        0.84705882, 0.66666667, 0.        ],\n",
              "       [0.00784314, 0.        , 0.        , 0.        , 0.25882353,\n",
              "        0.78431373, 0.87058824, 0.92941176, 0.9372549 , 0.94901961,\n",
              "        0.96470588, 0.95294118, 0.95686275, 0.86666667, 0.8627451 ,\n",
              "        0.75686275, 0.74901961, 0.70196078, 0.71372549, 0.71372549,\n",
              "        0.70980392, 0.69019608, 0.65098039, 0.65882353, 0.38823529,\n",
              "        0.22745098, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
              "        0.28235294, 0.16078431, 0.1372549 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2wuoCNBgbYA",
        "outputId": "3599a2b8-1624-440e-8db9-1b0d034bed64"
      },
      "source": [
        "# 입력 데이터 모양 전환\r\n",
        "# 60000*28*28 -> 60000*28*28*1(채널정보) 1:grayscale, 3:rgb\r\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\r\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\r\n",
        "\r\n",
        "# 훈련 데이터, 훈련 레이블, 검증 데이터, 검증 레이블\r\n",
        "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__qaiwMtd0KD",
        "outputId": "11961771-f1e5-4c3a-aced-63ee6fd05f41"
      },
      "source": [
        "# 라벨 데이터 One-hot encoding\r\n",
        "Y_train = np_utils.to_categorical(Y_train, 10)\r\n",
        "Y_test = np_utils.to_categorical(Y_test, 10)\r\n",
        "Y_train[0] # 9"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NTVmu5qkMZk"
      },
      "source": [
        "#### 3. CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN1pwwasiHzo",
        "outputId": "1580e581-73e6-49e1-a0e3-c0d8ad1759e0"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "# 28*28*1\r\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\r\n",
        "\r\n",
        "# filters=channel수\r\n",
        "# kernel_size는 가로, 세로 크기가 같으면 하나만 씀 -> 2*2\r\n",
        "# padding=same(입력과 출력 크기 동일)이면 K/2만큼 사방으로 해줌 -> 2/2 = 1\r\n",
        "# output_shape = (input_shape - kernel_size + 2 * padding_size) / stride + 1 = (28 - 2 + 2*1) / 1 = 28\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\n",
        "# 28*28*32\r\n",
        "# 파라미터 수 = (Kernel_size^2 * Input_Channels * Kernel_number) + Kernel_number\r\n",
        "#             = 2^2 * 1 * 32 + 32 = 160\r\n",
        "\r\n",
        "# pool_size는 가로, 세로 크기가 같으면 하나만 씀 -> 2*2\r\n",
        "# stride=None(default -> pool_size)\r\n",
        "# padding=valid(default)\r\n",
        "# output_shape = (input_shape - pool_size + 1) / strides = 28 - 2 + 1 / 2 = 27 / 2 = 13.5 => 14\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "# 14*14*32 (필터는 pooling에선 유지됨)\r\n",
        "\r\n",
        "# output_shape = (input_shape - kernel_size + 2 * padding_size) / stride + 1 = (14 - 2 + 2*1) / 1 = 14\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\r\n",
        "# 14*14*64\r\n",
        "# 파라미터 수 = (Kernel_size^2 * Input_Channels * Kernel_number) + Kernel_number\r\n",
        "#             = 2^2 * 32 * 64 + 64 = 8,256\r\n",
        "\r\n",
        "# output_shape = (input_shape - pool_size + 1) / strides = 14 - 2 + 1 / 2 = 13 / 2 = 6.5 => 7\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "# 7*7*64\r\n",
        "\r\n",
        "model.add(Flatten())\r\n",
        "# 7*7*64 = 3,136\r\n",
        "\r\n",
        "model.add(Dense(units=128, activation='relu'))\r\n",
        "# 파라미터 수: 3,136 * 128 + 128(bias) = 401,408 + 128 = 401,536\r\n",
        "\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "# 파라미터 수: 128 * 10 + 10(bias) = 1,280 + 10 = 1,290\r\n",
        "# 분류 문제이므로 softmax (확률값 출력)\r\n",
        "# 회귀 문제는 항등함수 사용\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Sr7td2wBtg"
      },
      "source": [
        "#### 4. Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm7X7hMukQK6",
        "outputId": "b1c94510-30a2-46e7-fb20-a8ee88dd6e4b"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "# 교차엔트로피 손실 값과 정확도를 넘겨줌\r\n",
        "\r\n",
        "begin = time()\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=MY_EPOCH, batch_size=MY_BATCH, verbose=1)\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "120/120 [==============================] - 8s 10ms/step - loss: 1.0609 - acc: 0.6453\n",
            "Epoch 2/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3984 - acc: 0.8594\n",
            "Epoch 3/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3497 - acc: 0.8750\n",
            "Epoch 4/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3222 - acc: 0.8844\n",
            "Epoch 5/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3046 - acc: 0.8923\n",
            "Epoch 6/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2846 - acc: 0.8985\n",
            "Epoch 7/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2644 - acc: 0.9043\n",
            "Epoch 8/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2457 - acc: 0.9136\n",
            "Epoch 9/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2426 - acc: 0.9130\n",
            "Epoch 10/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2290 - acc: 0.9174\n",
            "learning time: 19.06sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXspxi4H0Mq_"
      },
      "source": [
        "* 120/120: 120은 60,000개 훈련데이터 / 500개 배치 = 120번 가져와야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRpSrpFMxel8"
      },
      "source": [
        "#### 5. Model evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcQfJC4dwHy3",
        "outputId": "3fd72a98-6991-4a90-b40a-bdebb7b5dfbf"
      },
      "source": [
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print(\"Total loss: {:.2f}\".format(score[0]))\r\n",
        "# mse인 경우, rmse로 바꾸려면 score[0]에다가 sqrt 하면 됨\r\n",
        "print(\"Accuracy: {:.2f}\".format(score[1]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 75.0747 - acc: 0.7955\n",
            "Total loss: 75.07\n",
            "Accuracy: 0.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUAyW5uMyPm6"
      },
      "source": [
        "#### 6. Using model (Prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YLHqXlYwH_H",
        "outputId": "a1e2ede2-e30e-4f86-83d5-d1ed7b3f6f73"
      },
      "source": [
        "#                이미지숫자, width, height, channels\r\n",
        "image = X_test[0].reshape(1, 28, 28, 1) # 4차원으로 (batch 정보 추가)\r\n",
        "pred = model.predict(x=image)\r\n",
        "\r\n",
        "pred # 9"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o1M_oQIXnXk"
      },
      "source": [
        "* compile -> fit -> evaluate -> predict\r\n",
        "  * compile: 최적화(optimizer), 손실함수(loss)\r\n",
        "  * fit(학습 진행): epoch, batch 지정\r\n",
        "  * evaluate(평가): X_test, Y_test를 대상으로 진행\r\n",
        "  * predict(예측)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWu_lWayyx1z",
        "outputId": "c371a16f-9f8a-401a-d99d-5afca75d29bc"
      },
      "source": [
        "np.argmax(pred, axis=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E6INm5htzM6j",
        "outputId": "2f92b0a6-2eae-430d-8bda-14601d2054bd"
      },
      "source": [
        "img = X_test[0].reshape(28, 28) # 채널 정보를 넣었으니(CNN), 이를 다시 빼줘야 한다.\r\n",
        "img = (img * 255.0).astype(np.int) # 정규화 돌리기 (아까 255로 나눴음)\r\n",
        "print(img)\r\n",
        "plt.imshow(img, cmap='gray')\r\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0   765   255     0     0  1785\n",
            "      0  9435     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0   255   510     0  6885 21420  2805     0     0     0     0     0\n",
            "      0 30345     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0   255     0     0 22440 36465 28050     0     0     0     0  5610\n",
            "  23715 27030     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0  1020     0 13515 32895 30600 37485 44625 40035 42330 34425 39270\n",
            "  42840 35700     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "    510     0  2805 34935 33150 32640 40800 44880 40545 42585 45390 37995\n",
            "  38505 36720     0     0]\n",
            " [    0     0     0     0     0     0   255     0   510   255     0   765\n",
            "      0     0 29325 29070 27030 34935 42840 39015 39780 42075 42585 36465\n",
            "  40035 40290  2805     0]\n",
            " [    0     0     0     0   255     0     0     0     0     0   765     0\n",
            "      0 22695 35445 22950 23970 39015 37995 33405 38505 43095 43860 36465\n",
            "  40545 43095 12240     0]\n",
            " [    0     0     0     0     0     0   510  1020   255     0     0     0\n",
            "  24990 34680 28050 27795 28050 41310 34425 36720 37995 40545 42585 36720\n",
            "  40290 43095 30345     0]\n",
            " [    0     0   510   510   255   510     0     0     0     0  6630 27540\n",
            "  29835 25245 28305 29835 34680 39780 34170 39270 39270 39780 40800 35955\n",
            "  37485 39780 45390     0]\n",
            " [  765     0     0     0     0     0     0  5355 13515 23460 29835 28305\n",
            "  26265 29325 32895 34170 36465 39270 42075 43350 39270 38505 39270 36465\n",
            "  35190 38250 42075 10965]\n",
            " [    0     0  5865 13770 16575 19380 21675 30090 32640 31365 28305 28815\n",
            "  30090 32385 31875 35445 33915 34680 40800 35700 39525 41055 36720 39525\n",
            "  43860 41055 48195 15810]\n",
            " [    0 17340 23970 22950 28305 29070 28305 29070 29325 32385 34425 34680\n",
            "  36465 32130 32385 38505 39270 36465 37740 31875 41310 41310 36720 35190\n",
            "  39015 41310 49980 14790]\n",
            " [17850 43095 32895 26520 24990 25500 23970 24735 24990 26010 27540 27030\n",
            "  30345 30600 32895 37995 39780 42585 48450 48450 49980 50490 50490 47685\n",
            "  50235 48195 46920  9180]\n",
            " [ 4080 32130 43605 47940 47940 46920 43605 39015 34425 30600 32130 32385\n",
            "  37230 47175 49725 53295 53040 65025 53295 45135 62475 64260 64005 64005\n",
            "  62985 56100 52530 12495]\n",
            " [    0     0     0  3060 17085 27030 41820 47175 50745 53550 53805 53550\n",
            "  53040 48450 38250 20910  2040     0     0     0 45390 53040 47940 44625\n",
            "  41310 40290 38505  2805]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH6Uqn44ZdYK"
      },
      "source": [
        "#### 실습 문제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiiFrZLhZfNO",
        "outputId": "dd55fe9a-d376-4031-b8cc-dd739000b969"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=128, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "begin = time()\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=0, batch_size=MY_BATCH, verbose=1) # epochs = 0\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print(\"Total loss: {:.2f}\".format(score[0]))\r\n",
        "print(\"Accuracy: {:.2f}\".format(score[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "learning time: 0.13sec\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 30.5831 - acc: 0.0791\n",
            "Total loss: 30.77\n",
            "Accuracy: 0.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fvvboNhZ4aM",
        "outputId": "5c856951-2be0-4365-edad-48b12a267a33"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=128, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "begin = time()\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=20, batch_size=MY_BATCH, verbose=1) # epochs = 20\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print(\"Total loss: {:.2f}\".format(score[0]))\r\n",
        "print(\"Accuracy: {:.2f}\".format(score[1]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "120/120 [==============================] - 2s 10ms/step - loss: 1.0459 - acc: 0.6698\n",
            "Epoch 2/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4189 - acc: 0.8511\n",
            "Epoch 3/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3538 - acc: 0.8746\n",
            "Epoch 4/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3200 - acc: 0.8871\n",
            "Epoch 5/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2977 - acc: 0.8943\n",
            "Epoch 6/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2798 - acc: 0.8997\n",
            "Epoch 7/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2713 - acc: 0.9024\n",
            "Epoch 8/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2536 - acc: 0.9082\n",
            "Epoch 9/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2491 - acc: 0.9100\n",
            "Epoch 10/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2325 - acc: 0.9168\n",
            "Epoch 11/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2212 - acc: 0.9203\n",
            "Epoch 12/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2175 - acc: 0.9212\n",
            "Epoch 13/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2039 - acc: 0.9258\n",
            "Epoch 14/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2010 - acc: 0.9257\n",
            "Epoch 15/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1871 - acc: 0.9333\n",
            "Epoch 16/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1785 - acc: 0.9345\n",
            "Epoch 17/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1771 - acc: 0.9358\n",
            "Epoch 18/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1646 - acc: 0.9403\n",
            "Epoch 19/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1698 - acc: 0.9392\n",
            "Epoch 20/20\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1552 - acc: 0.9450\n",
            "learning time: 23.88sec\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 77.3763 - acc: 0.8302\n",
            "Total loss: 77.38\n",
            "Accuracy: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doiwd-1baF_5",
        "outputId": "66541d84-ed09-47a2-c7b9-79e5c5995926"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=128, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "begin = time()\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=100, batch_size=MY_BATCH, verbose=1) # epochs = 100\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print(\"Total loss: {:.2f}\".format(score[0]))\r\n",
        "print(\"Accuracy: {:.2f}\".format(score[1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "120/120 [==============================] - 2s 10ms/step - loss: 1.0117 - acc: 0.6538\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4091 - acc: 0.8564\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3427 - acc: 0.8789\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3330 - acc: 0.8831\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2934 - acc: 0.8968\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2716 - acc: 0.9015\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2559 - acc: 0.9068\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2441 - acc: 0.9110\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2362 - acc: 0.9157\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2254 - acc: 0.9193\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2156 - acc: 0.9231\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2020 - acc: 0.9277\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1987 - acc: 0.9284\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1897 - acc: 0.9307\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1851 - acc: 0.9331\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1798 - acc: 0.9345\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1671 - acc: 0.9396\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1642 - acc: 0.9413\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1565 - acc: 0.9429\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1490 - acc: 0.9452\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1429 - acc: 0.9497\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1381 - acc: 0.9510\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1320 - acc: 0.9517\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1285 - acc: 0.9540\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1291 - acc: 0.9514\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1177 - acc: 0.9588\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1118 - acc: 0.9603\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1071 - acc: 0.9625\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1006 - acc: 0.9646\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0990 - acc: 0.9646\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0951 - acc: 0.9661\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0877 - acc: 0.9690\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0859 - acc: 0.9700\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0766 - acc: 0.9730\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0767 - acc: 0.9728\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0786 - acc: 0.9722\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0670 - acc: 0.9767\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0635 - acc: 0.9781\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0644 - acc: 0.9771\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0566 - acc: 0.9807\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0581 - acc: 0.9801\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0535 - acc: 0.9817\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0497 - acc: 0.9827\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0487 - acc: 0.9835\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0372 - acc: 0.9888\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0465 - acc: 0.9846\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0399 - acc: 0.9864\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0320 - acc: 0.9901\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.9907\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0292 - acc: 0.9912\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0334 - acc: 0.9884\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0261 - acc: 0.9921\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0263 - acc: 0.9919\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0181 - acc: 0.9955\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0175 - acc: 0.9953\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0203 - acc: 0.9941\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0201 - acc: 0.9939\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0241 - acc: 0.9922\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0174 - acc: 0.9946\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0164 - acc: 0.9958\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0140 - acc: 0.9964\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0090 - acc: 0.9982\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0096 - acc: 0.9979\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0136 - acc: 0.9963\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0132 - acc: 0.9967\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0074 - acc: 0.9987\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0056 - acc: 0.9990\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0051 - acc: 0.9995\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0041 - acc: 0.9997\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0186 - acc: 0.9939\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0532 - acc: 0.9811\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0166 - acc: 0.9945\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0087 - acc: 0.9980\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0055 - acc: 0.9991\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0059 - acc: 0.9987\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0027 - acc: 0.9998\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 9.7442e-04 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 9.7385e-04 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 9.7214e-04 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 8.1128e-04 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0104 - acc: 0.9968\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0605 - acc: 0.9791\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0254 - acc: 0.9905\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0068 - acc: 0.9987\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0024 - acc: 0.9999\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 8.7880e-04 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 7.8007e-04 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 7.0269e-04 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 6.5676e-04 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 5.8897e-04 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 5.4377e-04 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 5.0967e-04 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 4.8543e-04 - acc: 1.0000\n",
            "learning time: 119.13sec\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 292.7824 - acc: 0.8340\n",
            "Total loss: 292.78\n",
            "Accuracy: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAjooRMZZdth",
        "outputId": "243eb199-26ae-4230-d4d6-9f5fffac962d"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=128, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc']) # optimizer=sgd\r\n",
        "\r\n",
        "begin = time()\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=MY_EPOCH, batch_size=MY_BATCH, verbose=1)\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print(\"Total loss: {:.2f}\".format(score[0]))\r\n",
        "print(\"Accuracy: {:.2f}\".format(score[1]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 2.2611 - acc: 0.2447\n",
            "Epoch 2/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 1.8188 - acc: 0.5446\n",
            "Epoch 3/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 1.0136 - acc: 0.6258\n",
            "Epoch 4/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.8311 - acc: 0.6838\n",
            "Epoch 5/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.7557 - acc: 0.7242\n",
            "Epoch 6/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.7065 - acc: 0.7448\n",
            "Epoch 7/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6915 - acc: 0.7459\n",
            "Epoch 8/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6511 - acc: 0.7638\n",
            "Epoch 9/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6273 - acc: 0.7712\n",
            "Epoch 10/10\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.6028 - acc: 0.7816\n",
            "learning time: 12.17sec\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 84.3497 - acc: 0.7483\n",
            "Total loss: 84.35\n",
            "Accuracy: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51hEN_ePc2I6"
      },
      "source": [
        "* adam\r\n",
        "* sgd\r\n",
        "* 최적화 함수: 가중치를 어떻게 보정하는지를 판단함. 즉, 전역 최솟값을 찾는 것이 목표!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5WacR2Zc1mi",
        "outputId": "422a7745-9b12-4824-f6ad-14696e303db5"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\r\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=128, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "begin = time()\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=MY_EPOCH, batch_size=16, verbose=1) #batch_size=16\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print(\"Total loss: {:.2f}\".format(score[0]))\r\n",
        "print(\"Accuracy: {:.2f}\".format(score[1]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.5421 - acc: 0.8071\n",
            "Epoch 2/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.2807 - acc: 0.8967\n",
            "Epoch 3/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.2314 - acc: 0.9154\n",
            "Epoch 4/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.1900 - acc: 0.9294\n",
            "Epoch 5/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.1614 - acc: 0.9397\n",
            "Epoch 6/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.1359 - acc: 0.9484\n",
            "Epoch 7/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.1195 - acc: 0.9544\n",
            "Epoch 8/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0964 - acc: 0.9641\n",
            "Epoch 9/10\n",
            "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0830 - acc: 0.9692\n",
            "Epoch 10/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0652 - acc: 0.9764\n",
            "learning time: 90.83sec\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 147.0027 - acc: 0.8000\n",
            "Total loss: 147.00\n",
            "Accuracy: 0.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtukJZvQpB8U",
        "outputId": "2e3483bc-dc2a-4891-a478-54ee61bcaa6b"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3])))\r\n",
        "model.add(Conv2D(filters=32, kernel_size=8, padding='same', activation='relu')) # kernel_size=8\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Conv2D(filters=64, kernel_size=8, padding='same', activation='relu')) # kernel_size=8\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=128, activation='relu'))\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "begin = time()\r\n",
        "model.fit(x=X_train, y=Y_train, epochs=MY_EPOCH, batch_size=MY_BATCH, verbose=1)\r\n",
        "end =time()\r\n",
        "print(\"learning time: {:.2f}sec\".format(end - begin))\r\n",
        "\r\n",
        "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\r\n",
        "print(\"Total loss: {:.2f}\".format(score[0]))\r\n",
        "print(\"Accuracy: {:.2f}\".format(score[1]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 28, 28, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 14, 14, 64)        131136    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 536,042\n",
            "Trainable params: 536,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "120/120 [==============================] - 3s 18ms/step - loss: 0.9708 - acc: 0.6606\n",
            "Epoch 2/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.4084 - acc: 0.8558\n",
            "Epoch 3/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.3371 - acc: 0.8809\n",
            "Epoch 4/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.2966 - acc: 0.8925\n",
            "Epoch 5/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.2693 - acc: 0.9019\n",
            "Epoch 6/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.2515 - acc: 0.9089\n",
            "Epoch 7/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.2356 - acc: 0.9144\n",
            "Epoch 8/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.2170 - acc: 0.9220\n",
            "Epoch 9/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.1999 - acc: 0.9275\n",
            "Epoch 10/10\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.1894 - acc: 0.9311\n",
            "learning time: 22.70sec\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 40.5019 - acc: 0.8913\n",
            "Total loss: 40.50\n",
            "Accuracy: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwzhYVgLqGwl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}